{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.utils import shuffle\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, \\\n",
    "                            r2_score, mean_poisson_deviance, mean_gamma_deviance, accuracy_score \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>1.013440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>1.002530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.998634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>1.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>2022-11-24</td>\n",
       "      <td>0.999421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>0.999578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>0.999574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>0.999604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>0.999660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Close\n",
       "54   2018-01-02  1.004900\n",
       "55   2018-01-03  1.013440\n",
       "56   2018-01-04  1.002530\n",
       "57   2018-01-05  0.998634\n",
       "58   2018-01-06  1.008990\n",
       "...         ...       ...\n",
       "1841 2022-11-24  0.999421\n",
       "1842 2022-11-25  0.999578\n",
       "1843 2022-11-26  0.999574\n",
       "1844 2022-11-27  0.999604\n",
       "1846 2022-11-29  0.999660\n",
       "\n",
       "[1792 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usdt = pd.read_csv('USDT.csv')\n",
    "df_usdt['Date'] = pd.to_datetime(df_usdt['Date'])\n",
    "df_usdt = df_usdt[['Date','Close']]\n",
    "df_usdt = df_usdt[df_usdt['Date'] > '2018-01-01']\n",
    "df_usdt = df_usdt.dropna()\n",
    "btc_stock = df_usdt.copy()\n",
    "btc_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_usdt['Date']\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_usdt = scaler.fit_transform(np.array(df_usdt).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test split\n",
    "training_size = int(len(df_usdt)*0.8)\n",
    "test_size = len(df_usdt) - training_size\n",
    "\n",
    "train_data, test_data = df_usdt[0:training_size,:], df_usdt[training_size:len(df_usdt),:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1417, 15)\n",
      "y_train:  (1417,)\n",
      "X_test:  (343, 15)\n"
     ]
    }
   ],
   "source": [
    "time_step = 15\n",
    "X_train_wof, y_train_wof = create_dataset(train_data, time_step)\n",
    "X_test_wof, y_test_wof = create_dataset(test_data, time_step)\n",
    "\n",
    "print(\"X_train: \", X_train_wof.shape)\n",
    "print(\"y_train: \", y_train_wof.shape)\n",
    "print(\"X_test: \", X_test_wof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1417, 15, 1)\n",
      "X_test:  (343, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_wof = X_train_wof.reshape(X_train_wof.shape[0],X_train_wof.shape[1] , 1)\n",
    "X_test_wof = X_test_wof.reshape(X_test_wof.shape[0],X_test_wof.shape[1] , 1)\n",
    "\n",
    "print(\"X_train: \", X_train_wof.shape)\n",
    "print(\"X_test: \", X_test_wof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 4s 61ms/step - loss: 0.0647 - val_loss: 0.0091\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0085 - val_loss: 1.5325e-04\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0041 - val_loss: 1.4672e-04\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0038 - val_loss: 3.2766e-04\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0037 - val_loss: 3.7115e-04\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0037 - val_loss: 3.2599e-04\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0037 - val_loss: 3.7047e-04\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0037 - val_loss: 3.4724e-04\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0037 - val_loss: 3.7661e-04\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0037 - val_loss: 3.4528e-04\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0037 - val_loss: 3.4031e-04\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0036 - val_loss: 3.0961e-04\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0036 - val_loss: 3.1040e-04\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0036 - val_loss: 3.3301e-04\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0036 - val_loss: 3.2520e-04\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0036 - val_loss: 3.4334e-04\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0036 - val_loss: 3.4960e-04\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0036 - val_loss: 3.5414e-04\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 3s 63ms/step - loss: 0.0036 - val_loss: 3.3716e-04\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0036 - val_loss: 3.4438e-04\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0035 - val_loss: 3.3876e-04\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0035 - val_loss: 3.6627e-04\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0035 - val_loss: 3.1361e-04\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0035 - val_loss: 3.3215e-04\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.0035 - val_loss: 3.1880e-04\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0035 - val_loss: 3.1233e-04\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0035 - val_loss: 3.2992e-04\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0035 - val_loss: 3.6724e-04\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0035 - val_loss: 3.5337e-04\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0035 - val_loss: 3.2770e-04\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0035 - val_loss: 3.1526e-04\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0035 - val_loss: 3.0332e-04\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0035 - val_loss: 3.3418e-04\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0035 - val_loss: 3.2990e-04\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0035 - val_loss: 3.1965e-04\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0035 - val_loss: 3.3501e-04\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0035 - val_loss: 3.7643e-04\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0035 - val_loss: 3.6112e-04\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0034 - val_loss: 3.1325e-04\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 3.3795e-04\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 3.8180e-04\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0034 - val_loss: 3.3176e-04\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0034 - val_loss: 3.5812e-04\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0034 - val_loss: 3.5437e-04\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0034 - val_loss: 3.3754e-04\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 3.0206e-04\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0034 - val_loss: 2.8778e-04\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0034 - val_loss: 3.0408e-04\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 2.8594e-04\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 2.9324e-04\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 2.7794e-04\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 2.8217e-04\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 3.5698e-04\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 2.9728e-04\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 3.0925e-04\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0034 - val_loss: 3.0087e-04\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 2.7281e-04\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0034 - val_loss: 2.8111e-04\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 3.1082e-04\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0034 - val_loss: 3.2578e-04\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0034 - val_loss: 3.1249e-04\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0033 - val_loss: 3.2673e-04\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0033 - val_loss: 3.4308e-04\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0033 - val_loss: 3.0642e-04\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 3.3284e-04\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0033 - val_loss: 3.1657e-04\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0033 - val_loss: 2.6567e-04\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0033 - val_loss: 3.4719e-04\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 3.1193e-04\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0033 - val_loss: 2.9142e-04\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0033 - val_loss: 2.5282e-04\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0033 - val_loss: 2.7142e-04\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0033 - val_loss: 2.8723e-04\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 3.0852e-04\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0033 - val_loss: 3.0594e-04\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 3.3211e-04\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 3.1307e-04\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0033 - val_loss: 2.8221e-04\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 3.0399e-04\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0033 - val_loss: 2.7278e-04\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 2.6621e-04\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0033 - val_loss: 2.4906e-04\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.6459e-04\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0032 - val_loss: 2.5244e-04\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0032 - val_loss: 2.8045e-04\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0032 - val_loss: 2.9123e-04\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.7574e-04\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0032 - val_loss: 2.9112e-04\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0032 - val_loss: 3.1532e-04\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.8836e-04\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0032 - val_loss: 2.6655e-04\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0032 - val_loss: 2.7271e-04\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0032 - val_loss: 2.8569e-04\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0032 - val_loss: 2.8439e-04\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.9223e-04\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.7855e-04\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.8116e-04\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.7879e-04\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0032 - val_loss: 2.8696e-04\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0032 - val_loss: 2.8326e-04\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0032 - val_loss: 2.4049e-04\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0032 - val_loss: 2.2846e-04\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0032 - val_loss: 2.7124e-04\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0031 - val_loss: 2.6232e-04\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0031 - val_loss: 2.5225e-04\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0031 - val_loss: 2.3967e-04\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0031 - val_loss: 2.8557e-04\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0031 - val_loss: 3.3936e-04\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0031 - val_loss: 2.8546e-04\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0031 - val_loss: 2.9332e-04\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0031 - val_loss: 2.5760e-04\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0031 - val_loss: 2.5888e-04\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0031 - val_loss: 2.3429e-04\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0031 - val_loss: 2.7136e-04\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0031 - val_loss: 2.6252e-04\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0031 - val_loss: 2.9222e-04\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0031 - val_loss: 2.9101e-04\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0031 - val_loss: 2.5402e-04\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0031 - val_loss: 2.5687e-04\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0031 - val_loss: 2.5387e-04\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0031 - val_loss: 2.4712e-04\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0031 - val_loss: 2.4599e-04\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0031 - val_loss: 3.1072e-04\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0030 - val_loss: 2.7524e-04\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.6722e-04\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.4352e-04\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.5174e-04\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.1896e-04\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0030 - val_loss: 2.6347e-04\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0030 - val_loss: 2.7123e-04\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0030 - val_loss: 2.6345e-04\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.6600e-04\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0030 - val_loss: 2.6963e-04\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.6829e-04\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.6767e-04\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.4311e-04\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.8588e-04\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0030 - val_loss: 2.3652e-04\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.1476e-04\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.6365e-04\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0030 - val_loss: 2.4470e-04\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0029 - val_loss: 2.3182e-04\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0029 - val_loss: 2.7127e-04\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0029 - val_loss: 2.4800e-04\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0029 - val_loss: 2.3571e-04\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0029 - val_loss: 2.4958e-04\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0029 - val_loss: 1.9706e-04\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0029 - val_loss: 2.3185e-04\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0029 - val_loss: 2.4086e-04\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0029 - val_loss: 2.9013e-04\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0029 - val_loss: 2.1295e-04\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0029 - val_loss: 2.3518e-04\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0029 - val_loss: 2.3751e-04\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0029 - val_loss: 2.0210e-04\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0029 - val_loss: 1.9043e-04\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0029 - val_loss: 2.3101e-04\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0029 - val_loss: 2.1588e-04\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0028 - val_loss: 2.4025e-04\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0028 - val_loss: 2.3231e-04\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0028 - val_loss: 2.1778e-04\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0028 - val_loss: 2.2925e-04\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0028 - val_loss: 2.2361e-04\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0028 - val_loss: 2.4080e-04\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0028 - val_loss: 2.2523e-04\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0028 - val_loss: 2.2916e-04\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0028 - val_loss: 2.1603e-04\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 3s 64ms/step - loss: 0.0028 - val_loss: 2.3670e-04\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0028 - val_loss: 2.1565e-04\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 2s 49ms/step - loss: 0.0028 - val_loss: 2.3211e-04\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0027 - val_loss: 2.3337e-04\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0027 - val_loss: 2.6631e-04\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 3s 69ms/step - loss: 0.0027 - val_loss: 3.1168e-04\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0027 - val_loss: 2.6286e-04\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0027 - val_loss: 2.4595e-04\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0027 - val_loss: 2.6946e-04\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0027 - val_loss: 2.3145e-04\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0026 - val_loss: 1.6270e-04\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0026 - val_loss: 1.6260e-04\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0026 - val_loss: 1.5237e-04\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0026 - val_loss: 1.5667e-04\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0026 - val_loss: 1.5721e-04\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0025 - val_loss: 1.4996e-04\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.0025 - val_loss: 1.3636e-04\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0025 - val_loss: 1.5511e-04\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0025 - val_loss: 1.4063e-04\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0025 - val_loss: 1.4574e-04\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 2s 49ms/step - loss: 0.0025 - val_loss: 1.3067e-04\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0025 - val_loss: 1.5261e-04\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0025 - val_loss: 1.2843e-04\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0025 - val_loss: 1.2115e-04\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0024 - val_loss: 1.4040e-04\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0024 - val_loss: 1.4813e-04\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0024 - val_loss: 1.3421e-04\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0024 - val_loss: 1.3484e-04\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0024 - val_loss: 1.5723e-04\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0024 - val_loss: 1.4828e-04\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0024 - val_loss: 1.2883e-04\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0024 - val_loss: 1.2979e-04\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0024 - val_loss: 1.2023e-04\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0024 - val_loss: 1.2037e-04\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.2037e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00012037358828820288"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wof=Sequential()\n",
    "model_wof.add(LSTM(10,input_shape=(None,1),activation=\"relu\"))\n",
    "model_wof.add(Dense(1))\n",
    "model_wof.compile(loss=\"mean_squared_error\",optimizer=\"sgd\")\n",
    "history = model_wof.fit(\n",
    "                    X_train_wof,\n",
    "                    y_train_wof,\n",
    "                    validation_data=(X_test_wof,y_test_wof),\n",
    "                    epochs=200,\n",
    "                    batch_size=32,\n",
    "                    verbose=1\n",
    "                    )\n",
    "model_wof.evaluate(X_test_wof, y_test_wof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1.005740</td>\n",
       "      <td>1.009880</td>\n",
       "      <td>0.993250</td>\n",
       "      <td>1.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>1.006600</td>\n",
       "      <td>1.023280</td>\n",
       "      <td>1.002640</td>\n",
       "      <td>1.013440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>1.013200</td>\n",
       "      <td>1.016180</td>\n",
       "      <td>0.993822</td>\n",
       "      <td>1.002530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>1.001750</td>\n",
       "      <td>1.005010</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.998634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>1.000280</td>\n",
       "      <td>1.012090</td>\n",
       "      <td>0.996847</td>\n",
       "      <td>1.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>2022-11-24</td>\n",
       "      <td>0.999191</td>\n",
       "      <td>0.999507</td>\n",
       "      <td>0.999123</td>\n",
       "      <td>0.999421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999590</td>\n",
       "      <td>0.999404</td>\n",
       "      <td>0.999578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.999626</td>\n",
       "      <td>0.999507</td>\n",
       "      <td>0.999574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>0.999568</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>0.999503</td>\n",
       "      <td>0.999604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>0.999681</td>\n",
       "      <td>0.999696</td>\n",
       "      <td>0.999606</td>\n",
       "      <td>0.999660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1792 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Open      High       Low     Close\n",
       "54   2018-01-02  1.005740  1.009880  0.993250  1.004900\n",
       "55   2018-01-03  1.006600  1.023280  1.002640  1.013440\n",
       "56   2018-01-04  1.013200  1.016180  0.993822  1.002530\n",
       "57   2018-01-05  1.001750  1.005010  0.985915  0.998634\n",
       "58   2018-01-06  1.000280  1.012090  0.996847  1.008990\n",
       "...         ...       ...       ...       ...       ...\n",
       "1841 2022-11-24  0.999191  0.999507  0.999123  0.999421\n",
       "1842 2022-11-25  0.999417  0.999590  0.999404  0.999578\n",
       "1843 2022-11-26  0.999577  0.999626  0.999507  0.999574\n",
       "1844 2022-11-27  0.999568  0.999654  0.999503  0.999604\n",
       "1846 2022-11-29  0.999681  0.999696  0.999606  0.999660\n",
       "\n",
       "[1792 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usdt = pd.read_csv('USDT.csv')\n",
    "df_usdt['Date'] = pd.to_datetime(df_usdt['Date'])\n",
    "df_usdt = df_usdt[['Date','Open','High','Low','Close']]\n",
    "df_usdt = df_usdt[df_usdt['Date'] > '2018-01-01']\n",
    "df_usdt = df_usdt.dropna()\n",
    "btc_stock = df_usdt.copy()\n",
    "btc_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_usdt['Date']\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_usdt = scaler.fit_transform(df_usdt)\n",
    "\n",
    "with open('scaler USDT.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test split\n",
    "training_size = int(len(df_usdt)*0.8)\n",
    "test_size = len(df_usdt) - training_size\n",
    "\n",
    "train_data, test_data = df_usdt[0:training_size,:], df_usdt[training_size:len(df_usdt),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step,-1])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1417, 15, 4)\n",
      "y_train:  (1417,)\n",
      "X_test:  (343, 15, 4)\n",
      "y_test:  (343,)\n"
     ]
    }
   ],
   "source": [
    "time_step = 15\n",
    "X_train_wf, y_train_wf = create_dataset(train_data, time_step)\n",
    "X_test_wf, y_test_wf = create_dataset(test_data, time_step)\n",
    "\n",
    "print(\"X_train: \", X_train_wf.shape)\n",
    "print(\"y_train: \", y_train_wf.shape)\n",
    "print(\"X_test: \", X_test_wf.shape)\n",
    "print(\"y_test: \", y_test_wf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 4s 66ms/step - loss: 0.1278 - val_loss: 0.0597\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0248 - val_loss: 1.6856e-04\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0025 - val_loss: 1.8032e-04\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0022 - val_loss: 2.6286e-04\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.0021 - val_loss: 7.6057e-05\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0021 - val_loss: 6.5823e-05\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0021 - val_loss: 7.0073e-05\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0020 - val_loss: 7.9847e-05\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0020 - val_loss: 1.7093e-04\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0020 - val_loss: 3.8791e-05\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0020 - val_loss: 1.0210e-04\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0020 - val_loss: 2.0035e-05\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0020 - val_loss: 5.0067e-05\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0020 - val_loss: 1.3218e-04\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0020 - val_loss: 2.0534e-05\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0019 - val_loss: 1.8079e-05\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0019 - val_loss: 5.5998e-05\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0019 - val_loss: 5.9852e-05\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0019 - val_loss: 4.6186e-05\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0019 - val_loss: 4.0674e-05\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0019 - val_loss: 3.6770e-05\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0019 - val_loss: 1.8376e-05\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0019 - val_loss: 1.9235e-05\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0019 - val_loss: 3.4567e-05\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0019 - val_loss: 1.4179e-05\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0019 - val_loss: 1.7944e-05\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 2s 50ms/step - loss: 0.0019 - val_loss: 1.4260e-05\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0019 - val_loss: 2.8235e-05\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0019 - val_loss: 1.4954e-05\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0019 - val_loss: 1.4459e-05\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0019 - val_loss: 7.8021e-05\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0019 - val_loss: 1.6682e-05\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 2.5328e-05\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0018 - val_loss: 4.5951e-05\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0019 - val_loss: 1.6132e-04\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0019 - val_loss: 2.1553e-05\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0018 - val_loss: 1.4333e-05\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 8.1285e-05\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0018 - val_loss: 1.5518e-05\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0018 - val_loss: 7.8211e-05\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.5921e-05\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0019 - val_loss: 2.1964e-04\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.5786e-05\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0018 - val_loss: 1.9359e-05\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 3.5032e-05\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.5215e-05\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 1.5462e-05\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.3634e-05\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.9125e-05\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.3404e-05\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0018 - val_loss: 2.3311e-05\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 3.2528e-05\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 4s 91ms/step - loss: 0.0018 - val_loss: 2.2219e-05\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 3s 70ms/step - loss: 0.0018 - val_loss: 1.3943e-05\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0018 - val_loss: 2.7094e-05\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0018 - val_loss: 2.8706e-05\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 3s 76ms/step - loss: 0.0018 - val_loss: 2.6332e-05\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 2s 47ms/step - loss: 0.0018 - val_loss: 7.7633e-05\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0018 - val_loss: 2.2557e-05\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0018 - val_loss: 1.6254e-04\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.7345e-05\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 9.8656e-05\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0019 - val_loss: 9.6066e-05\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 2.0488e-05\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.2977e-05\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0018 - val_loss: 3.2160e-05\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0018 - val_loss: 2.0488e-05\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 4.5606e-05\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 2.4090e-05\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 1.3873e-05\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 2.4827e-05\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.2607e-05\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0018 - val_loss: 1.5256e-05\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0018 - val_loss: 1.8366e-05\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 1.3895e-05\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 2.7346e-05\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 2.2601e-05\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.3822e-05\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 1.8415e-05\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0018 - val_loss: 4.9261e-05\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 2.0463e-05\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0018 - val_loss: 1.2394e-05\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.6344e-05\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0017 - val_loss: 7.5914e-05\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 1.9809e-05\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0018 - val_loss: 9.2950e-05\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0018 - val_loss: 5.1608e-05\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0017 - val_loss: 4.9461e-05\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0017 - val_loss: 3.7956e-05\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 1.9132e-05\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0018 - val_loss: 1.7134e-05\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0017 - val_loss: 1.3150e-05\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 1.4938e-05\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0018 - val_loss: 1.4030e-05\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 2s 53ms/step - loss: 0.0017 - val_loss: 8.1060e-05\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0018 - val_loss: 4.4302e-05\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 2.7901e-05\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 4.4609e-05\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.6931e-05\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.3260e-05\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 2.7829e-05\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 4.4396e-05\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.8755e-04\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0018 - val_loss: 1.2809e-05\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.2679e-05\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 5.4482e-05\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 3.7047e-05\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 1.9316e-05\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.8780e-05\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 7.8739e-05\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.3475e-04\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 2s 51ms/step - loss: 0.0017 - val_loss: 5.0986e-05\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 1.8652e-04\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0017 - val_loss: 2.7566e-05\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0018 - val_loss: 2.2266e-05\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 1.9223e-05\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0017 - val_loss: 3.4217e-05\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 4.9886e-05\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 1.9852e-05\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0017 - val_loss: 2.0759e-05\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 1.0530e-04\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 3.3454e-05\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.3959e-05\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0017 - val_loss: 2.7480e-05\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 6.3823e-05\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 1.8958e-04\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 1.8249e-05\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0017 - val_loss: 1.9124e-05\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 3.1144e-05\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 3s 64ms/step - loss: 0.0017 - val_loss: 3.1397e-05\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 6.0380e-05\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 1.5207e-05\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0017 - val_loss: 1.7020e-05\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 5.6713e-05\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0018 - val_loss: 1.4262e-05\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 3s 63ms/step - loss: 0.0017 - val_loss: 3.5689e-05\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 4.2518e-05\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 3.5831e-05\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0017 - val_loss: 2.4948e-05\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 2.0467e-05\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0017 - val_loss: 1.8774e-05\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 2.4928e-05\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0017 - val_loss: 2.8757e-05\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 3.3154e-05\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 2.7761e-05\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0017 - val_loss: 1.3246e-04\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0018 - val_loss: 1.0368e-04\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 3s 63ms/step - loss: 0.0017 - val_loss: 1.4241e-05\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.0017 - val_loss: 1.0295e-04\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0016 - val_loss: 2.1491e-05\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 2s 50ms/step - loss: 0.0017 - val_loss: 1.5089e-05\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0016 - val_loss: 2.4326e-05\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0016 - val_loss: 1.8059e-05\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0016 - val_loss: 1.4506e-05\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0017 - val_loss: 2.4780e-05\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 3s 63ms/step - loss: 0.0016 - val_loss: 2.4136e-05\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0017 - val_loss: 1.3830e-05\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 2.4568e-05\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0018 - val_loss: 1.2057e-04\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0017 - val_loss: 1.8067e-05\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0016 - val_loss: 8.8554e-05\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0017 - val_loss: 1.9423e-05\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0017 - val_loss: 3.5060e-05\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0017 - val_loss: 2.0289e-05\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0017 - val_loss: 3.1557e-05\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0017 - val_loss: 2.1675e-05\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0016 - val_loss: 3.5079e-05\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.0017 - val_loss: 5.7903e-05\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0017 - val_loss: 2.2753e-05\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0017 - val_loss: 1.4847e-05\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0016 - val_loss: 1.6679e-04\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.0017 - val_loss: 1.4631e-05\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 3s 66ms/step - loss: 0.0016 - val_loss: 1.3443e-05\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 3s 64ms/step - loss: 0.0016 - val_loss: 7.7244e-05\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 3s 66ms/step - loss: 0.0016 - val_loss: 1.5304e-04\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 3s 62ms/step - loss: 0.0017 - val_loss: 2.6747e-05\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.0017 - val_loss: 3.1718e-05\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 5.2400e-05\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 3s 64ms/step - loss: 0.0017 - val_loss: 2.2753e-05\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0016 - val_loss: 2.2999e-05\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0016 - val_loss: 2.9057e-05\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0016 - val_loss: 1.4601e-05\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0016 - val_loss: 1.8719e-05\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0016 - val_loss: 1.4962e-05\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0016 - val_loss: 6.5591e-05\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 2.0457e-05\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0016 - val_loss: 3.3165e-05\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0017 - val_loss: 1.3484e-05\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0017 - val_loss: 3.4729e-05\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0016 - val_loss: 6.2681e-05\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0017 - val_loss: 2.7305e-05\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0016 - val_loss: 1.3952e-05\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0016 - val_loss: 1.6206e-05\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 3s 56ms/step - loss: 0.0016 - val_loss: 1.6939e-05\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.0016 - val_loss: 3.8643e-05\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0016 - val_loss: 7.8169e-05\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0016 - val_loss: 4.5202e-05\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0016 - val_loss: 4.2457e-05\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 2s 54ms/step - loss: 0.0017 - val_loss: 1.1790e-04\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 3s 59ms/step - loss: 0.0016 - val_loss: 1.9063e-05\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9063e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9062750652665272e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wf=Sequential()\n",
    "model_wf.add(LSTM(10,input_shape=(None,4),activation=\"relu\"))\n",
    "model_wf.add(Dense(1))\n",
    "model_wf.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "history = model_wf.fit(\n",
    "                    X_train_wf,\n",
    "                    y_train_wf,\n",
    "                    validation_data=(X_test_wf,y_test_wf),\n",
    "                    epochs=200,\n",
    "                    batch_size=32,\n",
    "                    verbose=1\n",
    "                    )\n",
    "model_wf.evaluate(X_test_wf, y_test_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wf.save('model_wf USDT.h5')\n",
    "model_wof.save('model_wof USDT.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4b521e29a846470c96e928a1c4aafac58a12234cdaa98f9ca60bc431873fee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
